{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e3afe6",
   "metadata": {},
   "source": [
    "# Trust Region Example\n",
    "\n",
    "A sample usage of the STEAM library for testing various trust-region solvers on a divergent (for Gauss-Newton)\n",
    "error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import numpy as np\n",
    "np.set_printoptions(6, suppress=True)\n",
    "\n",
    "from pysteam import evaluator, problem, state, solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple error metric to test trust region methods. Implements a vector error in $\\mathbb{R}^2$ with\n",
    "\n",
    "$$\n",
    "\\mathbf{e} = \\begin{bmatrix} x+1 \\\\ -2x^2 + x - 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Minimum error is at zero. Notably vanilla Gauss Newton is unable to converge to the answer as a step near zero causes it to diverge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DivergenceErrorEval(evaluator.Evaluator):\n",
    "\n",
    "  def __init__(self, state_vec: state.VectorSpaceStateVar) -> None:\n",
    "    super().__init__()\n",
    "    assert state_vec.get_perturb_dim() == 1\n",
    "    self._state_vec = state_vec\n",
    "\n",
    "  def is_active(self) -> bool:\n",
    "    return not self._state_vec.is_locked()\n",
    "\n",
    "  def evaluate(self, lhs: typing.Optional[np.ndarray] = None):\n",
    "    x = self._state_vec.get_value()[0, 0]\n",
    "    eval = np.empty((2, 1))\n",
    "    eval[0, 0] = x + 1.0\n",
    "    eval[1, 0] = -2.0 * x * x + x - 1.0\n",
    "\n",
    "    if lhs is None:\n",
    "      return eval\n",
    "\n",
    "    jacs = dict()\n",
    "\n",
    "    if not self._state_vec.is_locked():\n",
    "      jac = np.empty((2, 1))\n",
    "      jac[0, 0] = 1.0\n",
    "      jac[1, 0] = -4.0 * x + 1.0\n",
    "      jacs = {self._state_vec.get_key(): lhs @ jac}\n",
    "\n",
    "    return eval, jacs\n",
    "\n",
    "\n",
    "def setup_divergence_problem():\n",
    "  # Create vector state variable\n",
    "  initial = np.array([[10.0]])\n",
    "  state_var = state.VectorSpaceStateVar(initial)\n",
    "\n",
    "  # Setup noise model and loss function\n",
    "  noise_model = problem.StaticNoiseModel(np.eye(2))\n",
    "  loss_func = problem.L2LossFunc()\n",
    "  error_func = DivergenceErrorEval(state_var)\n",
    "  cost_term = problem.WeightedLeastSquareCostTerm(error_func, noise_model, loss_func)\n",
    "\n",
    "  # Initialize problem\n",
    "  opt_prob = problem.OptimizationProblem()\n",
    "  opt_prob.add_state_var(state_var)\n",
    "  opt_prob.add_cost_term(cost_term)\n",
    "\n",
    "  return opt_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Optimization\n",
      "------------------\n",
      "Number of States:  1\n",
      "Number of Cost Terms:  1\n",
      "Initial Cost:  18301.0\n",
      "Iteration:    1  -  Cost:  1165.4162\n",
      "Iteration:    2  -  Cost:    79.8090\n",
      "Iteration:    3  -  Cost:     7.6464\n",
      "Iteration:    4  -  Cost:     1.6152\n",
      "Iteration:    5  -  Cost:     4.2988\n",
      "Iteration:    6  -  Cost:     1.0289\n",
      "Iteration:    7  -  Cost:     1.0416\n",
      "Iteration:    8  -  Cost:     1.6583\n",
      "Iteration:    9  -  Cost:     1.0277\n",
      "Iteration:   10  -  Cost:     1.3380\n",
      "Iteration:   11  -  Cost:     1.0526\n",
      "Iteration:   12  -  Cost:     1.9861\n",
      "Iteration:   13  -  Cost:     1.0116\n",
      "Iteration:   14  -  Cost:     1.0948\n",
      "Iteration:   15  -  Cost:     1.0634\n",
      "Iteration:   16  -  Cost:     2.3784\n",
      "Iteration:   17  -  Cost:     1.0022\n",
      "Iteration:   18  -  Cost:     1.0119\n",
      "Iteration:   19  -  Cost:     1.0244\n",
      "Iteration:   20  -  Cost:     1.2767\n",
      "Iteration:   21  -  Cost:     1.0578\n",
      "Iteration:   22  -  Cost:     2.1688\n",
      "Iteration:   23  -  Cost:     1.0061\n",
      "Iteration:   24  -  Cost:     1.0409\n",
      "Iteration:   25  -  Cost:     1.0490\n",
      "Iteration:   26  -  Cost:     1.8732\n",
      "Iteration:   27  -  Cost:     1.0162\n",
      "Iteration:   28  -  Cost:     1.1504\n",
      "Iteration:   29  -  Cost:     1.0655\n",
      "Iteration:   30  -  Cost:     2.4612\n",
      "Iteration:   31  -  Cost:     1.0012\n",
      "Iteration:   32  -  Cost:     1.0061\n",
      "Iteration:   33  -  Cost:     1.0151\n",
      "Iteration:   34  -  Cost:     1.1366\n",
      "Iteration:   35  -  Cost:     1.0655\n",
      "Iteration:   36  -  Cost:     2.4639\n",
      "Iteration:   37  -  Cost:     1.0012\n",
      "Iteration:   38  -  Cost:     1.0060\n",
      "Iteration:   39  -  Cost:     1.0148\n",
      "Iteration:   40  -  Cost:     1.1330\n",
      "Iteration:   41  -  Cost:     1.0655\n",
      "Iteration:   42  -  Cost:     2.4626\n",
      "Iteration:   43  -  Cost:     1.0012\n",
      "Iteration:   44  -  Cost:     1.0061\n",
      "Iteration:   45  -  Cost:     1.0150\n",
      "Iteration:   46  -  Cost:     1.1347\n",
      "Iteration:   47  -  Cost:     1.0655\n",
      "Iteration:   48  -  Cost:     2.4633\n",
      "Iteration:   49  -  Cost:     1.0012\n",
      "Iteration:   50  -  Cost:     1.0060\n",
      "Iteration:   51  -  Cost:     1.0149\n",
      "Iteration:   52  -  Cost:     1.1337\n",
      "Iteration:   53  -  Cost:     1.0655\n",
      "Iteration:   54  -  Cost:     2.4630\n",
      "Iteration:   55  -  Cost:     1.0012\n",
      "Iteration:   56  -  Cost:     1.0060\n",
      "Iteration:   57  -  Cost:     1.0149\n",
      "Iteration:   58  -  Cost:     1.1342\n",
      "Iteration:   59  -  Cost:     1.0655\n",
      "Iteration:   60  -  Cost:     2.4632\n",
      "Iteration:   61  -  Cost:     1.0012\n",
      "Iteration:   62  -  Cost:     1.0060\n",
      "Iteration:   63  -  Cost:     1.0149\n",
      "Iteration:   64  -  Cost:     1.1340\n",
      "Iteration:   65  -  Cost:     1.0655\n",
      "Iteration:   66  -  Cost:     2.4631\n",
      "Iteration:   67  -  Cost:     1.0012\n",
      "Iteration:   68  -  Cost:     1.0060\n",
      "Iteration:   69  -  Cost:     1.0149\n",
      "Iteration:   70  -  Cost:     1.1341\n",
      "Iteration:   71  -  Cost:     1.0655\n",
      "Iteration:   72  -  Cost:     2.4631\n",
      "Iteration:   73  -  Cost:     1.0012\n",
      "Iteration:   74  -  Cost:     1.0060\n",
      "Iteration:   75  -  Cost:     1.0149\n",
      "Iteration:   76  -  Cost:     1.1340\n",
      "Iteration:   77  -  Cost:     1.0655\n",
      "Iteration:   78  -  Cost:     2.4631\n",
      "Iteration:   79  -  Cost:     1.0012\n",
      "Iteration:   80  -  Cost:     1.0060\n",
      "Iteration:   81  -  Cost:     1.0149\n",
      "Iteration:   82  -  Cost:     1.1341\n",
      "Iteration:   83  -  Cost:     1.0655\n",
      "Iteration:   84  -  Cost:     2.4631\n",
      "Iteration:   85  -  Cost:     1.0012\n",
      "Iteration:   86  -  Cost:     1.0060\n",
      "Iteration:   87  -  Cost:     1.0149\n",
      "Iteration:   88  -  Cost:     1.1340\n",
      "Iteration:   89  -  Cost:     1.0655\n",
      "Iteration:   90  -  Cost:     2.4631\n",
      "Iteration:   91  -  Cost:     1.0012\n",
      "Iteration:   92  -  Cost:     1.0060\n",
      "Iteration:   93  -  Cost:     1.0149\n",
      "Iteration:   94  -  Cost:     1.1340\n",
      "Iteration:   95  -  Cost:     1.0655\n",
      "Iteration:   96  -  Cost:     2.4631\n",
      "Iteration:   97  -  Cost:     1.0012\n",
      "Iteration:   98  -  Cost:     1.0060\n",
      "Iteration:   99  -  Cost:     1.0149\n",
      "Iteration:  100  -  Cost:     1.1340\n",
      "Termination Cause:  MAX ITERATIONS\n",
      "Total Optimization Time: 0.1030 seconds\n",
      "Gauss Newton terminates from: MAX ITERATIONS after 100 iterations.\n"
     ]
    }
   ],
   "source": [
    "gauss_newton = solver.GaussNewtonSolver(setup_divergence_problem(), verbose=True, max_iterations=100)\n",
    "gauss_newton.optimize()\n",
    "print(\"Gauss Newton terminates from:\", gauss_newton.termination_cause, \"after\", gauss_newton.curr_iteration,\n",
    "      \"iterations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Optimization\n",
      "------------------\n",
      "Number of States:  1\n",
      "Number of Cost Terms:  1\n",
      "Initial Cost:  18301.0\n",
      "Iteration:    1  -  Cost:  1165.4162  -  Search Coeff:  1.000\n",
      "Iteration:    2  -  Cost:    79.8090  -  Search Coeff:  1.000\n",
      "Iteration:    3  -  Cost:     7.6464  -  Search Coeff:  1.000\n",
      "Iteration:    4  -  Cost:     1.6152  -  Search Coeff:  1.000\n",
      "Iteration:    5  -  Cost:     1.0583  -  Search Coeff:  0.500\n",
      "Iteration:    6  -  Cost:     1.0561  -  Search Coeff:  1.000\n",
      "Iteration:    7  -  Cost:     1.0009  -  Search Coeff:  0.250\n",
      "Iteration:    8  -  Cost:     1.0002  -  Search Coeff:  0.500\n",
      "Iteration:    9  -  Cost:     1.0000  -  Search Coeff:  0.500\n",
      "Iteration:   10  -  Cost:     1.0000  -  Search Coeff:  0.500\n",
      "Termination Cause:  CONVERGED ABSOLUTE CHANGE\n",
      "Total Optimization Time: 0.0100 seconds\n",
      "Line Search GN terminates from: CONVERGED ABSOLUTE CHANGE after 10 iterations.\n"
     ]
    }
   ],
   "source": [
    "ls_gauss_newton = solver.LineSearchGaussNewtonSolver(setup_divergence_problem(), verbose=True, max_iterations=100)\n",
    "ls_gauss_newton.optimize()\n",
    "print(\"Line Search GN terminates from:\", ls_gauss_newton.termination_cause, \"after\", ls_gauss_newton.curr_iteration, \"iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Optimization\n",
      "------------------\n",
      "Number of States:  1\n",
      "Number of Cost Terms:  1\n",
      "Initial Cost:  18301.0\n",
      "Iteration:    1  -  Cost:  1165.4167  -  TR Shrink:  0.000  -  AvP Ratio:  0.937\n",
      "Iteration:    2  -  Cost:    79.8091  -  TR Shrink:  0.000  -  AvP Ratio:  0.937\n",
      "Iteration:    3  -  Cost:     7.6464  -  TR Shrink:  0.000  -  AvP Ratio:  0.936\n",
      "Iteration:    4  -  Cost:     1.6152  -  TR Shrink:  0.000  -  AvP Ratio:  0.933\n",
      "Iteration:    5  -  Cost:     1.0583  -  TR Shrink:  7.000  -  AvP Ratio:  0.477\n",
      "Iteration:    6  -  Cost:     1.0001  -  TR Shrink:  1.000  -  AvP Ratio:  0.611\n",
      "Iteration:    7  -  Cost:     1.0000  -  TR Shrink:  1.000  -  AvP Ratio:  0.318\n",
      "Termination Cause:  CONVERGED ABSOLUTE CHANGE\n",
      "Total Optimization Time: 0.0129 seconds\n",
      "Levenberg–Marquardt terminates from: CONVERGED ABSOLUTE CHANGE after 7 iterations.\n"
     ]
    }
   ],
   "source": [
    "lev_marq = solver.LevMarqGaussNewtonSolver(setup_divergence_problem(), verbose=True, max_iterations=100)\n",
    "lev_marq.optimize()\n",
    "print(\"Levenberg–Marquardt terminates from:\", lev_marq.termination_cause, \"after\", lev_marq.curr_iteration,\n",
    "      \"iterations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Optimization\n",
      "------------------\n",
      "Number of States:  1\n",
      "Number of Cost Terms:  1\n",
      "Initial Cost:  18301.0\n",
      "Iteration:    1  -  Cost:  1165.4162  -  TR Shrink:  0.000  -  AvP Ratio:  0.937  -  Dogleg Segment: Gauss Newton   \n",
      "Iteration:    2  -  Cost:    79.8090  -  TR Shrink:  0.000  -  AvP Ratio:  0.937  -  Dogleg Segment: Gauss Newton   \n",
      "Iteration:    3  -  Cost:     7.6464  -  TR Shrink:  0.000  -  AvP Ratio:  0.936  -  Dogleg Segment: Gauss Newton   \n",
      "Iteration:    4  -  Cost:     1.6152  -  TR Shrink:  0.000  -  AvP Ratio:  0.933  -  Dogleg Segment: Gauss Newton   \n",
      "Iteration:    5  -  Cost:     1.0039  -  TR Shrink:  5.000  -  AvP Ratio:  0.657  -  Dogleg Segment: Grad Descent   \n",
      "Iteration:    6  -  Cost:     1.0013  -  TR Shrink:  3.000  -  AvP Ratio:  0.278  -  Dogleg Segment: Grad Descent   \n",
      "Iteration:    7  -  Cost:     1.0002  -  TR Shrink:  1.000  -  AvP Ratio:  0.411  -  Dogleg Segment: Grad Descent   \n",
      "Iteration:    8  -  Cost:     1.0000  -  TR Shrink:  2.000  -  AvP Ratio:  0.639  -  Dogleg Segment: Grad Descent   \n",
      "Iteration:    9  -  Cost:     1.0000  -  TR Shrink:  3.000  -  AvP Ratio:  0.415  -  Dogleg Segment: Grad Descent   \n",
      "Termination Cause:  CONVERGED ABSOLUTE CHANGE\n",
      "Total Optimization Time: 0.0109 seconds\n",
      "Powell's Dogleg terminates from: CONVERGED ABSOLUTE CHANGE after 9 iterations.\n"
     ]
    }
   ],
   "source": [
    "dl_gauss_newton = solver.DoglegGaussNewtonSolver(setup_divergence_problem(), verbose=True, max_iterations=100)\n",
    "dl_gauss_newton.optimize()\n",
    "print(\"Powell's Dogleg terminates from:\", dl_gauss_newton.termination_cause, \"after\", dl_gauss_newton.curr_iteration,\n",
    "      \"iterations.\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c15f067fe7e8127fdbb642962bd2cd8db04f16ef0be4627ba876d2142abc64bf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
